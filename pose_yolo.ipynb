{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\ragha\\Downloads\\walk.jpg: 448x640 2 persons, 178.7ms\n",
      "Speed: 3.1ms preprocess, 178.7ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolo11s-pose.pt')\n",
    "results = model(\"C:/Users/ragha/Downloads/walk.jpg\")\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[443.1198,  85.8227],\n",
      "         [447.7185,  79.2029],\n",
      "         [442.4140,  81.1137],\n",
      "         [467.3758,  76.6470],\n",
      "         [  0.0000,   0.0000],\n",
      "         [495.2459, 113.2120],\n",
      "         [443.9509, 114.0830],\n",
      "         [514.7013, 166.3592],\n",
      "         [435.6700, 165.4144],\n",
      "         [475.0921, 174.8191],\n",
      "         [426.5097, 213.6855],\n",
      "         [480.0145, 214.1777],\n",
      "         [444.9682, 213.6636],\n",
      "         [490.3084, 280.7159],\n",
      "         [437.5168, 284.3105],\n",
      "         [496.0169, 343.0397],\n",
      "         [443.9121, 359.1465]],\n",
      "\n",
      "        [[377.7271, 106.0822],\n",
      "         [379.6922, 101.6130],\n",
      "         [371.8454, 101.6037],\n",
      "         [  0.0000,   0.0000],\n",
      "         [358.0769, 106.2273],\n",
      "         [390.2489, 141.9727],\n",
      "         [344.7025, 141.3718],\n",
      "         [404.8810, 183.9051],\n",
      "         [339.4091, 181.4463],\n",
      "         [415.3259, 213.8278],\n",
      "         [348.7749, 199.7777],\n",
      "         [381.4799, 225.2374],\n",
      "         [356.3035, 225.0658],\n",
      "         [372.0657, 287.8157],\n",
      "         [371.4810, 284.4595],\n",
      "         [357.0729, 350.8577],\n",
      "         [389.8847, 341.3055]]])\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    xy = result.keypoints.xy\n",
    "    print(xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 134.2ms\n",
      "Speed: 1.6ms preprocess, 134.2ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 110.9ms\n",
      "Speed: 1.7ms preprocess, 110.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 112.6ms\n",
      "Speed: 2.6ms preprocess, 112.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 103.8ms\n",
      "Speed: 1.7ms preprocess, 103.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 126.1ms\n",
      "Speed: 2.2ms preprocess, 126.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 139.8ms\n",
      "Speed: 4.1ms preprocess, 139.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 161.9ms\n",
      "Speed: 7.2ms preprocess, 161.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 116.2ms\n",
      "Speed: 3.4ms preprocess, 116.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 98.3ms\n",
      "Speed: 1.4ms preprocess, 98.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 136.9ms\n",
      "Speed: 2.4ms preprocess, 136.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 151.4ms\n",
      "Speed: 2.5ms preprocess, 151.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 113.7ms\n",
      "Speed: 2.3ms preprocess, 113.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 150.0ms\n",
      "Speed: 2.0ms preprocess, 150.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 161.3ms\n",
      "Speed: 1.7ms preprocess, 161.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 193.8ms\n",
      "Speed: 2.2ms preprocess, 193.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 212.0ms\n",
      "Speed: 3.1ms preprocess, 212.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 209.8ms\n",
      "Speed: 2.8ms preprocess, 209.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 98.4ms\n",
      "Speed: 1.4ms preprocess, 98.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 119.3ms\n",
      "Speed: 2.3ms preprocess, 119.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 230.2ms\n",
      "Speed: 3.6ms preprocess, 230.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 196.5ms\n",
      "Speed: 3.5ms preprocess, 196.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 197.7ms\n",
      "Speed: 2.6ms preprocess, 197.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 186.4ms\n",
      "Speed: 4.1ms preprocess, 186.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 123.1ms\n",
      "Speed: 2.3ms preprocess, 123.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 100.8ms\n",
      "Speed: 2.2ms preprocess, 100.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 173.7ms\n",
      "Speed: 2.2ms preprocess, 173.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 197.6ms\n",
      "Speed: 3.1ms preprocess, 197.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 120.0ms\n",
      "Speed: 1.5ms preprocess, 120.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 95.3ms\n",
      "Speed: 2.3ms preprocess, 95.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "from ultralytics import YOLO\n",
    "\n",
    "video = cv.VideoCapture(\"C:/Users/ragha/Downloads/jhbj.mp4\")\n",
    "model = YOLO('yolo11s-pose.pt')\n",
    "while True:\n",
    "    ret , frame = video.read()\n",
    "\n",
    "    if ret:\n",
    "        results = model(frame)\n",
    "\n",
    "        annoated_frame = results[0].plot()\n",
    "\n",
    "        annoated_frame = cv.resize(annoated_frame , (1000,500))\n",
    "\n",
    "        cv.imshow('Boy and Dad' , annoated_frame)\n",
    "\n",
    "    if cv.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv.destroyAllWindows()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolo11s-seg.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\ragha\\Downloads\\walk.jpg: 448x640 2 persons, 1 car, 235.0ms\n",
      "Speed: 1.3ms preprocess, 235.0ms inference, 6.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Results saved to \u001b[1mc:\\Users\\ragha\\runs\\segment\\predict2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = model(\"C:/Users/ragha/Downloads/walk.jpg\" , save = True)\n",
    "results[0].show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 2 persons, 1 bench, 131.1ms\n",
      "Speed: 1.5ms preprocess, 131.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 109.8ms\n",
      "Speed: 1.5ms preprocess, 109.8ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 159.6ms\n",
      "Speed: 1.7ms preprocess, 159.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 129.4ms\n",
      "Speed: 1.6ms preprocess, 129.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 133.4ms\n",
      "Speed: 1.7ms preprocess, 133.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 benchs, 257.9ms\n",
      "Speed: 2.8ms preprocess, 257.9ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 benchs, 163.2ms\n",
      "Speed: 3.1ms preprocess, 163.2ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 137.9ms\n",
      "Speed: 1.4ms preprocess, 137.9ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 129.9ms\n",
      "Speed: 3.0ms preprocess, 129.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 2 benchs, 99.8ms\n",
      "Speed: 1.4ms preprocess, 99.8ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 139.3ms\n",
      "Speed: 2.5ms preprocess, 139.3ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 123.7ms\n",
      "Speed: 1.6ms preprocess, 123.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 224.3ms\n",
      "Speed: 3.2ms preprocess, 224.3ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 123.5ms\n",
      "Speed: 3.3ms preprocess, 123.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 157.6ms\n",
      "Speed: 3.2ms preprocess, 157.6ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 146.6ms\n",
      "Speed: 2.0ms preprocess, 146.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 108.2ms\n",
      "Speed: 2.8ms preprocess, 108.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 99.2ms\n",
      "Speed: 1.6ms preprocess, 99.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 115.5ms\n",
      "Speed: 2.5ms preprocess, 115.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 126.4ms\n",
      "Speed: 3.0ms preprocess, 126.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 214.0ms\n",
      "Speed: 3.7ms preprocess, 214.0ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 120.8ms\n",
      "Speed: 1.8ms preprocess, 120.8ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 113.1ms\n",
      "Speed: 1.5ms preprocess, 113.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 131.2ms\n",
      "Speed: 2.5ms preprocess, 131.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 141.4ms\n",
      "Speed: 2.7ms preprocess, 141.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 113.4ms\n",
      "Speed: 1.7ms preprocess, 113.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 128.7ms\n",
      "Speed: 2.4ms preprocess, 128.7ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 186.1ms\n",
      "Speed: 1.8ms preprocess, 186.1ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 178.5ms\n",
      "Speed: 1.9ms preprocess, 178.5ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 186.0ms\n",
      "Speed: 2.8ms preprocess, 186.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 128.1ms\n",
      "Speed: 2.8ms preprocess, 128.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 110.3ms\n",
      "Speed: 3.0ms preprocess, 110.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 118.7ms\n",
      "Speed: 3.4ms preprocess, 118.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 127.5ms\n",
      "Speed: 2.3ms preprocess, 127.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 142.2ms\n",
      "Speed: 2.2ms preprocess, 142.2ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 108.4ms\n",
      "Speed: 2.5ms preprocess, 108.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 137.7ms\n",
      "Speed: 1.6ms preprocess, 137.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 107.1ms\n",
      "Speed: 1.4ms preprocess, 107.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 110.6ms\n",
      "Speed: 1.6ms preprocess, 110.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 104.5ms\n",
      "Speed: 1.9ms preprocess, 104.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 102.6ms\n",
      "Speed: 2.2ms preprocess, 102.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 107.3ms\n",
      "Speed: 2.1ms preprocess, 107.3ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 97.2ms\n",
      "Speed: 1.4ms preprocess, 97.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 143.8ms\n",
      "Speed: 3.7ms preprocess, 143.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 119.9ms\n",
      "Speed: 1.5ms preprocess, 119.9ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 199.9ms\n",
      "Speed: 3.5ms preprocess, 199.9ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 141.6ms\n",
      "Speed: 2.6ms preprocess, 141.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 110.9ms\n",
      "Speed: 2.1ms preprocess, 110.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 211.0ms\n",
      "Speed: 2.6ms preprocess, 211.0ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 156.6ms\n",
      "Speed: 2.9ms preprocess, 156.6ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 117.1ms\n",
      "Speed: 1.4ms preprocess, 117.1ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 210.9ms\n",
      "Speed: 2.1ms preprocess, 210.9ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 146.4ms\n",
      "Speed: 2.1ms preprocess, 146.4ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 120.4ms\n",
      "Speed: 1.9ms preprocess, 120.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 140.4ms\n",
      "Speed: 2.8ms preprocess, 140.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 107.7ms\n",
      "Speed: 1.9ms preprocess, 107.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 103.1ms\n",
      "Speed: 1.6ms preprocess, 103.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 126.5ms\n",
      "Speed: 2.6ms preprocess, 126.5ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 118.6ms\n",
      "Speed: 2.7ms preprocess, 118.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 123.5ms\n",
      "Speed: 2.5ms preprocess, 123.5ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 140.5ms\n",
      "Speed: 3.1ms preprocess, 140.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 109.2ms\n",
      "Speed: 2.1ms preprocess, 109.2ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 110.0ms\n",
      "Speed: 3.1ms preprocess, 110.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 103.6ms\n",
      "Speed: 2.9ms preprocess, 103.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 98.3ms\n",
      "Speed: 1.4ms preprocess, 98.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 103.2ms\n",
      "Speed: 1.9ms preprocess, 103.2ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 persons, 1 bench, 154.1ms\n",
      "Speed: 2.6ms preprocess, 154.1ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2 as cv\n",
    "from ultralytics import YOLO\n",
    "\n",
    "video = cv.VideoCapture(\"C:/Users/ragha/Downloads/jhbj.mp4\")\n",
    "model = YOLO('yolo11s-seg.pt')\n",
    "while True:\n",
    "    ret , frame = video.read()\n",
    "\n",
    "    if ret:\n",
    "        results = model(frame)\n",
    "\n",
    "        annoated_frame = results[0].plot()\n",
    "\n",
    "        annoated_frame = cv.resize(annoated_frame , (1000,500))\n",
    "\n",
    "        cv.imshow('Boy and Dad' , annoated_frame)\n",
    "\n",
    "    if cv.waitKey(25) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "video.release()\n",
    "cv.destroyAllWindows()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oriented Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\ragha\\Downloads\\harbour.jpg: 704x1024 571.3ms\n",
      "Speed: 13.0ms preprocess, 571.3ms inference, 39.1ms postprocess per image at shape (1, 3, 704, 1024)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('yolo11s-obb.pt')\n",
    "results = model(\"C:/Users/ragha/Downloads/harbour.jpg\")\n",
    "results[0].show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
